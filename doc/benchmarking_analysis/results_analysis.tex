\documentclass[12pt]{article}
\title{Results Analysis}
\author{Mostafa Rizk}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage[margin=1.5in]{geometry}
\usepackage{fancyhdr}
\usepackage{url}
\usepackage{tabu}
\usepackage[title]{appendix}

\fancyhead{}
\fancyfoot{}
\fancyfoot[R]{\thepage}

\pagestyle{fancy}
\begin{document}
\maketitle
\tableofcontents

\section{Executive Summary}

We conducted an analysis of the SlopeForaging problem using Oller et al's rwg benchmarking approach to understand the types of challenges it poses and why it is difficult to evolve cooperative behaviour. Our main insights were:

\begin{itemize}
\item The SlopeForaging task has similar attributes to MountainCar, meaning a highly exploratory algorithm may be more successful.

\item SlopeForaging is more difficult than MountainCar due to observation size, action space, partial observability and the cooperative nature of the task. Memory appears to be required for good performance but bias neurons don't make a difference. A non-linear activation may be necessary for better results.

\item Sliding speed doesn't significantly impact the success of generalist solutions because changing sliding speed doesn't make agents slower or increase the cost of moving up the slope.

\item The absence of a cost for moving and carrying (i.e. a battery) does not make it harder to find generalist solutions.

\item The implementation contains some minor logical errors with regards to reward allocation and movement costs that should be modified for future experiments.

\item Networks with non-linear activation are not as successful as those with linear activation but get better with additional layers. This is most pronounced for tanh activation.

\end{itemize}

\section{Analysis}
		
We have faced difficulty evolving cooperation in the SlopeForaging multi-agent setting. We conducted an analysis using Oller et al's rwg benchmarking approach \cite{oller:AAMAS:2020} to understand the nature of the difficulties posed by this environment. We hope this will provide insight into what changes need to be made to our approach in order to produce the desired behaviour. All experiments used a homogeneous team, rewarded at the team level. Heterogeneous teams and individual rewards are not considered yet.\\

In each experiment, we sampled 50,000 genomes from a normal distribution with a mean of 0 and unit variance. Each genome was evaluated for 5 episodes. In any instance where hidden layers were used, we used 4 hidden units. We plot a) all the trials in sorted order, b) the variance of all trials and c) the distribution on a log scale. Details of the experimental setup can be found in the appendix.
		
\subsection{FFNN vs RNN} \label{FFNNvRNN}

We started our experiments with the simplest possible architecture, a FFNN with no bias, no hidden layers and linear activation (i.e. the identity function). We added up to two hidden layers, then repeated with bias, making for a total of 6 experiments. We then repeated the 6 experiments with a RNN.\\

For the FFNN, the scores are overwhelmingly poor, all falling into the leftmost bucket of the distribution plot as we see in Figure \ref{fig:ffnn_linear}. We see that the mean curve is very flat, with very little variance. The results appear to be almost identical regardless of the number of hidden layers. We discovered later, according to Theorem 1.5.1 in \cite{aggarwal:Springer:2018}, that hidden layers do not add computational power if the activation function is linear. We therefore chose to exclude these plots and repeated these experiments in Section \ref{non-linear} with various non-linear activations. We also find that adding a bias neuron does not make a difference to the results.\\

Most significantly, we see that an RNN performs overwhelmingly better than a FFNN. This makes sense because the environment is partially observable; an agent can only see 1 tile in any direction. A recurrent architecture means an agent can act on sequences of observations rather than individual observations, giving it an advantage over an agent with no memory. Presumably, if the agent's sensing radius was expanded to include the entire arena then the FFNN would perform similarly to the RNN.\\ 

Most of the RNN solutions are still quite poor, with the majority again fitting in the leftmost bucket in the distribution histogram \footnote{The highest scores approach 40,000 but the corresponding bucket in the histogram appears empty. This is because the y-axis begins at $10^0$, so if only one sample had a mean score in a particular bucket, that bucket is will not be visible on the histogram}. The mean curve is, as with the FFNN, very flat as we see in Figure \ref{fig:rnn_linear} but there is a spike at the end, denoting a very small proportion of samples that perform much better than the others. We can also see that the variance is higher for higher means which indicates that genomes that score well do not do so consistently across all episodes.\\

On a more fundamental level, we learn that the environment, independent of architecture, is challenging in a way that is similar to the MountainCar \cite{MountainCar} environment investigated by Oller et al \cite{oller:AAMAS:2020}. The plots are very similar with most solutions being equally poor and very few demonstrating a successful behaviour. This is likely because both environments have a sparse reward. In the case of MountainCar, an agent must reach the top of the mountain to receive a score better than -200 and there are no rewards for partially successful intermediate behaviours. An agent that never moves is equivalent to one that stops just short of the goal. In SlopeForaging, an agent that travels up the slope, picks up a resource, carries it to the bottom of the slope and drops it just short of the nest is equivalent to an agent that oscillates back and forth between two tiles. A non-negative score can only be achieved if a resource is fully retrieved and there are no rewards for partially successful intermediate behaviours, so the task can not be learned incrementally.\\

In fact, the SlopeForaging task is actually more difficult for the following reasons:

\begin{itemize}
\item \textit{The observation and action spaces are larger}- An observation consists of 41 binary inputs and there are 6 discrete actions. Even the simplest FFNN would have 41x6=246 weights compared to (2x4) + (4x4) + (4x3) = 36 for the most complex controller used by Oller et al to solve MountainCar. 

\item \textit{The environment is partially observable}- Partial observability means a more complex architecture (RNN) is required.

\item \textit{The best solution isn't found even after 50,000 samples}- In MountainCar \cite{MountainCar}, the theoretical best score is close to 0 and the solutions generated by Oller et al approach this value, whereas in SlopeForaging, a team of specialists hardcoded with a cooperative strategy scores 157,000 but the best mean score of the solutions found by rwg is 39,879 and the best episode score is 52,642 (for a recurrent network). This is very far from the best known solution. It is even far from the best known generalist strategy, which scores 112,000. And all this is with 5 times more sampled solutions than Oller et al. All this is to say, the task is difficult to solve and, according to Oller et al's recommendations, may require an algorithm with a strong exploratory component.

\end{itemize}

\textbf{Main takeaway: The SlopeForaging task has similar attributes to MountainCar but is more difficult due to observation size, action space, partial observability and the cooperative nature of the task. Memory appears to be required for good performance but bias doesn't make a difference. A non-linear activation may be necessary for better results.}

\begin{figure}[!tbp]
  \centering
  % FFNN no-bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_0HL/ffnn_no-bias_0HL_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_0HL/ffnn_no-bias_0HL_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_0HL/ffnn_no-bias_0HL_all_scores_log_dist_.png}}
  
% FFNN bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_0HL/ffnn_bias_0HL_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_0HL/ffnn_bias_0HL_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_0HL/ffnn_bias_0HL_all_scores_log_dist_.png}}
  
  \caption{FFNN with linear activation}
  \label{fig:ffnn_linear}
\end{figure}

%%%

\begin{figure}[!tbp]
  \centering
  % RNN no-bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL/rnn_no-bias_0HL_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL/rnn_no-bias_0HL_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL/rnn_no-bias_0HL_all_scores_log_dist_.png}}
  
% RNN bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL/rnn_bias_0HL_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL/rnn_bias_0HL_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL/rnn_bias_0HL_all_scores_log_dist_.png}}
  
  \caption{RNN with linear activation}
  \label{fig:rnn_linear}	
\end{figure}	

%%%
	
						
\subsection{Modifying Environmental Factors}

The initial experiments gave us an understanding of why the task is difficult, but they used a specific configuration of the environment. It was important to observe how changing these parameters changed the task difficulty. We varied a) the sliding speed, an approximation of slope angle for a 2D discrete environment, b) the presence of a battery i.e. the cost associated with performing actions, and c) the multi-agent nature.

\subsubsection{Sliding speed}

By default, the environment has sliding speed = 4. This means that when a resource is dropped, it slides 4 tiles per time step. This simulates a slope. The presence of a slope is the environmental factor that makes specialised teams outperform generalist teams. In an environment with no slope, a specialist strategy adds no benefit over a generalist one and may even be worse. Conversely, in an environment with a steeper slope, a specialist strategy could provide an even larger advantage because the slope can get resources to the bottom even faster. We ran experiments with varying sliding speed to observe how it impacts the solutions found. For brevity, we only use one architecture: an RNN with bias, no hidden layers and linear activation.\\

From the plots in Figure \ref{fig:sliding_speed} we find that the sliding speed doesn't make much of a difference to the quality of solutions. It is important to note that while sliding speed is an approximation of slope angle it is not a direct analogue. Slope angle affects the speed at which a resource slides down the slope and the speed at which an agent goes up, but sliding speed only affects the speed of the resource. Since the only successful solutions rwg finds are generalists, the speed at which resources slide doesn't factor into their scores because they never drop resources. Any solutions that do drop resources, don't succeed at the task, so the speed at which those resources slid did not affect their score.\\

We also spotted a minor logical error in the code that causes the cost of moving up/down the slope to be the same regardless of the sliding speed. This does not change the stag hunt dynamic of the environment, however it would more accurately represent a slope if the cost of moving changed with the sliding speed.\\ 

\textbf{Main takeaway: Sliding speed doesn't significantly impact the success of generalist solutions because changing sliding speed doesn't make agents slower or increase the cost of moving up the slope.}

\begin{figure}[!tbp]
  \centering
  % Sliding speed 0
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_0/sliding_0_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_0/sliding_0_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_0/sliding_0_all_scores_log_dist_.png}}
  
  % Sliding speed 1
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_1/sliding_1_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_1/sliding_1_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_1/sliding_1_all_scores_log_dist_.png}}
  
  % Sliding speed 2
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_2/sliding_2_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_2/sliding_2_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_2/sliding_2_all_scores_log_dist_.png}}
  
  % Sliding speed 3
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_3/sliding_3_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_3/sliding_3_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_3/sliding_3_all_scores_log_dist_.png}}
  
  % Sliding speed 5
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_5/sliding_5_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_5/sliding_5_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_5/sliding_5_all_scores_log_dist_.png}}
  
   % Sliding speed 6
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_6/sliding_6_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_6/sliding_6_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{sliding_6/sliding_6_all_scores_log_dist_.png}}
  
  \caption{Varying sliding speed}
  \label{fig:sliding_speed}
\end{figure}

\subsubsection{No "battery''}

In our default setup, agents pay a cost for moving in any direction. The cost is higher going up the slope and lower coming down the slope. It is also higher if carrying a resource. This simulates the presence of a battery. An agent is thus, penalised for wasting time and for carrying things on the slope, making specialisation more advantageous and more accurately representing a real robot. A battery is not used by Ferrante et al. \cite{ferrante:2015:PLoSCB} and simply counting the number of resources is sufficient to see specialisation emerge, however, Ferrante et al also had the benefit of 3D physics to create delays in robot and resource movement. It is important to understand how the penalisation of movement impacts the scores of solutions.\\

In the no battery experiments, movement has no cost and agents are rewarded one point for retrieving a resource rather than 1000. We scale down the mean and variance axes in our plots by 1000 to account for this difference. Once again we see in Figure \ref{fig:no_battery} that the majority of solutions are poor, with most retrieving 0 resources. And again we see that variance is higher for the better solutions indicating they are not always successful. There would appear to be more solutions that fall into larger buckets but this is likely only because the scale of the y-axis is smaller, so it is easier for those buckets to be filled. The experiment may need to be repeated with a 1000 score for each retrieved resource.\\

Overall, the absence of a ''battery" or cost to movement does not appear to make any difference to the difficulty of the task which makes sense as the main source of difficulty is the sparseness of the reward. We also know from hardcoded controllers that in the absence of a battery, a specialist team still outperforms a generalist team (scoring 158.4 as opposed to 114.4). The presence or absence of movement and carrying costs does not appear to affect task difficulty.\\

\textbf{Main takeaway: The presence or absence of a ''battery" does not appear to affect task difficulty.}

\begin{figure}[!tbp]
  \centering
  % No battery
  \subfloat{\includegraphics[width=0.3\textwidth]{no_battery/no_battery_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{no_battery/no_battery_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{no_battery/no_battery_all_scores_log_dist_.png}}
  
  \caption{No battery}
  \label{fig:no_battery}
\end{figure}

\subsubsection{Single agent}

In the single agent experiment, we again used an RNN with bias, no hidden layers and linear activation. Here the results are significantly lower, with the top score reaching only 6153 (Table \ref{tab:environment_comparison}), compared to a 2-agent team which achieves a top score of 39,082. This can be explained by the reward function. When an agent retrieves a resource, it receives +1000 and its teammates do as well. The team score is the sum of all individual scores. This mean that if there are two agents, one retrieved resource counts as 2000 and with two agents acting on the environment, they are likely to gather twice as many resources for four times the score. This is a logical error that was spotted as a result of these experiments.\\

In terms of task difficulty, having only one agent does not impact the difficulty. Figure \ref{fig:single_agent} shows similar patterns to previous plots, but this may be a factor if there are enough agents in the arena to experience collisions. Sequential position updates are one way of solving that problem.\\

\textbf{Main takeaway: Rewards should be divided not multiplied}

\begin{figure}[!tbp]
  \centering
  % Single Agent
  \subfloat{\includegraphics[width=0.3\textwidth]{single_agent/single_agent_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{single_agent/single_agent_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{single_agent/single_agent_all_scores_log_dist_.png}}
  
  \caption{Single agent}
  \label{fig:single_agent}
\end{figure}

\subsection{Non-linear Activation}\label{non-linear}

Following the results from Section \ref{FFNNvRNN} we conducted the same experiments with non-linear activations, specifically tanh, ReLU and sigmoid functions. We primarily looked at RNNs because previous experiments indicated that memory was likely necessary for successful behaviour, but we included a FFNN with tanh activation for due diligence. For the three activations, as before, we did six experiments, three with bias and three without, and of those three we tested 0, 1 and 2 hidden layers, with 4 hidden units.\\

For all three activations, the highest scores are lower than they are for linear activation. This is surprising as one would expect nonlinear activation to provide more representational power to the neural network. Interestingly, though, for tanh and ReLU (Figures \ref{fig:rnn_tanh} and \ref{fig:rnn_relu} respectively) the instances of high scores (and the top score) increase as hidden layers are added. This suggests that while a network with linear activation is more successful with no hidden layers than its counterparts, the networks with tanh and ReLU activation gain representational power and their performance improves with additional layers. In the histograms in Figure \ref{fig:rnn_relu}, the difference between layers is not as visible as it is for Figure \ref{fig:rnn_tanh} but can be seen in the top scores in Table \ref{tab:architecture_comparison}. This suggests that the added gain of each new layer is greater for networks using tanh than networks using ReLU.\\

In both cases, though, it is unclear how many layers this trend continues for, or if varying the number of hidden units would accelerate the improvement or if there is another activation function for which the improvement is more pronounced. Additional rwg runs with these deeper architectures could help answer these questions but may also reach a bottleneck as additional layers means more weights, increasing the size of the solution space. Some form of topological search \cite{stanley:MIT:2002} is another way of discovering which architecture is most capable of representing problem solutions.\\ 

Sigmoid activation was the exception to the trend, with the distribution of RNN solutions in Figure \ref{fig:rnn_sigmoid} being similar to those of FFNNs with linear activation in Figure \ref{fig:ffnn_linear}. Networks with sigmoid activation appear to not have the representational power required to solve the problem, but it is unclear why. The same holds true for FFNNs with tanh activation (Figure \ref{fig:ffnn_tanh}). For all networks, though, bias neurons did not make a notable difference.\\

\textbf{Main takeaway: Networks with non-linear activation are not as successful as those with linear activation but get better with additional layers. This is most pronounced for tanh activation. }


\begin{figure}[!tbp]
  \centering
  % RNN no-bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_tanh/rnn_no-bias_0HL_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_tanh/rnn_no-bias_0HL_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_tanh/rnn_no-bias_0HL_tanh_all_scores_log_dist_.png}}
  
  % RNN no-bias 1HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_tanh/rnn_no-bias_1HL_4HU_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_tanh/rnn_no-bias_1HL_4HU_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_tanh/rnn_no-bias_1HL_4HU_tanh_all_scores_log_dist_.png}}

% RNN no-bias 2HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_tanh/rnn_no-bias_2HL_4HU_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_tanh/rnn_no-bias_2HL_4HU_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_tanh/rnn_no-bias_2HL_4HU_tanh_all_scores_log_dist_.png}}
  
% RNN bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_tanh/rnn_bias_0HL_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_tanh/rnn_bias_0HL_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_tanh/rnn_bias_0HL_tanh_all_scores_log_dist_.png}}
  
  % RNN bias 1HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_tanh/rnn_bias_1HL_4HU_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_tanh/rnn_bias_1HL_4HU_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_tanh/rnn_bias_1HL_4HU_tanh_all_scores_log_dist_.png}}

% RNN bias 2HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_tanh/rnn_bias_2HL_4HU_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_tanh/rnn_bias_2HL_4HU_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_tanh/rnn_bias_2HL_4HU_tanh_all_scores_log_dist_.png}}  
  
  \caption{RNN with tanh activation}
  \label{fig:rnn_tanh}	
\end{figure}	

\begin{figure}[!tbp]
  \centering
  % RNN no-bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_relu/rnn_no-bias_0HL_relu_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_relu/rnn_no-bias_0HL_relu_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_relu/rnn_no-bias_0HL_relu_all_scores_log_dist_.png}}
  
  % RNN no-bias 1HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_relu/rnn_no-bias_1HL_4HU_relu_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_relu/rnn_no-bias_1HL_4HU_relu_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_relu/rnn_no-bias_1HL_4HU_relu_all_scores_log_dist_.png}}

% RNN no-bias 2HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_relu/rnn_no-bias_2HL_4HU_relu_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_relu/rnn_no-bias_2HL_4HU_relu_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_relu/rnn_no-bias_2HL_4HU_relu_all_scores_log_dist_.png}}
  
% RNN bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_relu/rnn_bias_0HL_relu_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_relu/rnn_bias_0HL_relu_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_relu/rnn_bias_0HL_relu_all_scores_log_dist_.png}}
  
  % RNN bias 1HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_relu/rnn_bias_1HL_4HU_relu_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_relu/rnn_bias_1HL_4HU_relu_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_relu/rnn_bias_1HL_4HU_relu_all_scores_log_dist_.png}}

% RNN bias 2HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_relu/rnn_bias_2HL_4HU_relu_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_relu/rnn_bias_2HL_4HU_relu_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_relu/rnn_bias_2HL_4HU_relu_all_scores_log_dist_.png}}  
  
  \caption{RNN with ReLU activation}
  \label{fig:rnn_relu}	
\end{figure}	

\begin{figure}[!tbp]
  \centering
  % RNN no-bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_sigmoid/rnn_no-bias_0HL_sigmoid_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_sigmoid/rnn_no-bias_0HL_sigmoid_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_0HL_sigmoid/rnn_no-bias_0HL_sigmoid_all_scores_log_dist_.png}}
  
  % RNN no-bias 1HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_sigmoid/rnn_no-bias_1HL_4HU_sigmoid_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_sigmoid/rnn_no-bias_1HL_4HU_sigmoid_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_1HL_4HU_sigmoid/rnn_no-bias_1HL_4HU_sigmoid_all_scores_log_dist_.png}}

% RNN no-bias 2HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_sigmoid/rnn_no-bias_2HL_4HU_sigmoid_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_sigmoid/rnn_no-bias_2HL_4HU_sigmoid_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_no-bias_2HL_4HU_sigmoid/rnn_no-bias_2HL_4HU_sigmoid_all_scores_log_dist_.png}}
  
% RNN bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_sigmoid/rnn_bias_0HL_sigmoid_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_sigmoid/rnn_bias_0HL_sigmoid_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_0HL_sigmoid/rnn_bias_0HL_sigmoid_all_scores_log_dist_.png}}
  
  % RNN bias 1HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_sigmoid/rnn_bias_1HL_4HU_sigmoid_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_sigmoid/rnn_bias_1HL_4HU_sigmoid_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_1HL_4HU_sigmoid/rnn_bias_1HL_4HU_sigmoid_all_scores_log_dist_.png}}

% RNN bias 2HL
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_sigmoid/rnn_bias_2HL_4HU_sigmoid_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_sigmoid/rnn_bias_2HL_4HU_sigmoid_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{rnn_bias_2HL_4HU_sigmoid/rnn_bias_2HL_4HU_sigmoid_all_scores_log_dist_.png}}  
  
  \caption{RNN with sigmoid activation}
  \label{fig:rnn_sigmoid}	
\end{figure}	

\begin{figure}[!tbp]
  \centering
  % FFNN no-bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_0HL_tanh/ffnn_no-bias_0HL_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_0HL_tanh/ffnn_no-bias_0HL_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_0HL_tanh/ffnn_no-bias_0HL_tanh_all_scores_log_dist_.png}}
  
  % FFNN no-bias 1HL
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_1HL_4HU_tanh/ffnn_no-bias_1HL_4HU_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_1HL_4HU_tanh/ffnn_no-bias_1HL_4HU_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_1HL_4HU_tanh/ffnn_no-bias_1HL_4HU_tanh_all_scores_log_dist_.png}}

% FFNN no-bias 2HL
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_2HL_4HU_tanh/ffnn_no-bias_2HL_4HU_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_2HL_4HU_tanh/ffnn_no-bias_2HL_4HU_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_no-bias_2HL_4HU_tanh/ffnn_no-bias_2HL_4HU_tanh_all_scores_log_dist_.png}}
  
% FFNN bias 0HL
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_0HL_tanh/ffnn_bias_0HL_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_0HL_tanh/ffnn_bias_0HL_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_0HL_tanh/ffnn_bias_0HL_tanh_all_scores_log_dist_.png}}
  
  % FFNN bias 1HL
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_1HL_4HU_tanh/ffnn_bias_1HL_4HU_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_1HL_4HU_tanh/ffnn_bias_1HL_4HU_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_1HL_4HU_tanh/ffnn_bias_1HL_4HU_tanh_all_scores_log_dist_.png}}

% FFNN bias 2HL
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_2HL_4HU_tanh/ffnn_bias_2HL_4HU_tanh_score_trials_ordered_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_2HL_4HU_tanh/ffnn_bias_2HL_4HU_tanh_variance_meanscore_.png}}
  \hfill
  \subfloat{\includegraphics[width=0.3\textwidth]{ffnn_bias_2HL_4HU_tanh/ffnn_bias_2HL_4HU_tanh_all_scores_log_dist_.png}}  
  
  \caption{FFNN with tanh activation}
  \label{fig:ffnn_tanh}	
\end{figure}

\begin{table}
\begin{center}
\begin{tabu}{ |c|c|c|c|c| } 
 \hline
 Network Type & Layers & Bias & Activation & Top Score\\
 \tabucline[1.5pt]{-}
 FFNN & 0 & N & Linear & -620 \\
 \hline
 FFNN & 0 & Y & Linear & -620 \\
 \tabucline[1.5pt]{-}
 
 RNN & 0 & N & Linear & 39879 \\
 \hline
 RNN & 0 & Y & Linear & 39879 \\
 \tabucline[1.5pt]{-}
 
 RNN & 0 & N & tanh & 297 \\
 \hline
 RNN & 1 & N & tanh & 3361 \\
 \hline
 RNN & 2 & N & tanh & 8376 \\
 \hline
 RNN & 0 & Y & tanh & 297 \\
 \hline
 RNN & 1 & Y & tanh & 3361 \\
 \hline
 RNN & 2 & Y & tanh & 8376 \\
 \tabucline[1.5pt]{-}
 
 RNN & 0 & N & ReLU & 1375 \\
 \hline
 RNN & 1 & N & ReLU & 2402 \\
 \hline
 RNN & 2 & N & ReLU & 5224 \\
 \hline
 RNN & 0 & Y & ReLU & 1375 \\
 \hline
 RNN & 1 & Y & ReLU & 2402 \\
 \hline
 RNN & 2 & Y & ReLU & 5224 \\
 \tabucline[1.5pt]{-}
 
 RNN & 0 & N & sigmoid & -610 \\
 \hline
 RNN & 1 & N & sigmoid & -603 \\
 \hline
 RNN & 2 & N & sigmoid & -620 \\
 \hline
 RNN & 0 & Y & sigmoid & -610 \\
 \hline
 RNN & 1 & Y & sigmoid & -603 \\
 \hline
 RNN & 2 & Y & sigmoid & -620 \\
 \tabucline[1.5pt]{-}

 FFNN & 0 & N & tanh & -620 \\
 \hline
 FFNN & 1 & N & tanh & -609 \\
 \hline
 FFNN & 2 & N & tanh & -620 \\
 \hline
 FFNN & 0 & Y & tanh & -620 \\
 \hline
 FFNN & 1 & Y & tanh & -609 \\
 \hline
 FFNN & 2 & Y & tanh & -620 \\
 \tabucline[1.5pt]{-}
 \hline
\end{tabu}
\end{center}	
\caption{\label{tab:architecture_comparison} Top scores with different architectures}
\end{table}

\begin{table}
\begin{center}
\begin{tabu}{ |c|c| } 
 \hline
 Environment & Top Score\\
 \tabucline[1.5pt]{-}
 
Sliding speed = 0 & 39082 \\
 \hline
Sliding speed = 1 & 39879 \\
 \hline
 Sliding speed = 2 & 39879 \\
 \hline
 Sliding speed = 3 & 39879 \\
 \hline
 Sliding speed = 4 & 39879 \\
 \hline
 Sliding speed = 5 & 39879 \\
 \hline
 Sliding speed = 6 & 39879 \\
 \tabucline[1.5pt]{-}

 No battery & 41.2 \\
 \tabucline[1.5pt]{-}
 
Single agent & 6153 \\
 \tabucline[1.5pt]{-}



 \hline
\end{tabu}
\end{center}	
\caption{\label{tab:environment_comparison} Top scores in different environments}
\end{table}

\bibliographystyle{plain}
\bibliography{references}

\begin{appendices}

In the following sections we explain the current experimental setup for reference.

\section{Experimental Setup}

\subsection{The Foraging Task}\label{task_description}

We use a foraging task as the testbed for the evolution of specialisation.
The task is modelled off the foraging behaviour of the Atta Leafcutter ant, as in Ferrante et al \cite{ferrante:PLOS_CB:2015} and Pini et al \cite{pini:ICSI:2012, pini:Swarm_Intelligence:2011}.
In nature, the ants cut leaves from a tree and take them to their nest. 
Sometimes the ants partition the task. 
When they do so, some ants are droppers, who cut leaves and let them fall, while other ants are collectors who collect fallen leaves from the base of the tree and take them to the nest.
This partitioned approach is advantageous because gravity transports leaves faster than ants can.
Rather than every ant climbing up the tree, cutting a leaf and bringing it to the nest, when they partition the ants are able to transport more leaves in the same time-span while consuming less energy.
We model this scenario similarly to Ferrante et al \cite{ferrante:PLOS_CB:2015}, with a slope in place of the tree trunk. 
Pini et al use a slightly different implementation where there resources can be transported via a long corridor or a booth with a wait time \cite{pini:ICSI:2012, pini:Swarm_Intelligence:2011}. 
Pini et al's approach may be useful for comparison at a later stage of the research.
Much like the ants, agents transport resources from the source to the nest.
An agent on a team can use a generalist strategy where it acts individually, going up and down the slope to retrieve resources.
An agent can also use a specialist strategy. 
As a specialist it can either be a dropper, going up the slope once and dropping things from the nest, or it can be a collector and gather the resources that accumulate at the base of the slope, called the cache.
Much like real robots that deplete their battery and ants that deplete their energy stores, there is a cost to moving and it is compounded when going up the slope. 
Teams that use complementary specialist strategies pay a smaller energy cost as well as time cost, since resources slide down faster than they can be carried. 
Preliminary analysis with hard-coded agents verifies that specialist teams gain higher overall reward than generalist teams.\\

More formally, you have a team of $n_{agents}$ agents. 
They are placed in a rectangular arena that is $l$ tiles long and $w$ tiles wide, illustrated in Figure \ref{fig:arena} and Figure \ref{fig:arena_2}. 
The arena is divided into four sections $l= l_{nest} + l_{cache} + l_{slope} + l_{source}$.
Each episode is composed of a number of finite time-steps $t=0, .... T$. 
Since 3D physics is computationally expensive we use a 2D environment to expedite our experiments as the focus of our research is on the evolutionary process and team dynamics rather than the robotic element.
We also use a discrete scenario, as opposed to a continuous one for further simplicity.
To simulate the presence of gravity, resources move when on the slope, at a speed greater than the agents are capable of.
The high sliding speed creates evolutionary pressure for the team to specialise.
Agents travel at speed $s_{agent}$ and resources slide when placed on the slope with a speed of $s_{resource}$ 
Additionally agents pay costs for moving.
This simulates the presence of a battery, with energy expenditure varying depending on where the agent is moving.
There is a base cost to moving $c$ paid by the agent for moving in any direction.
The base cost is multiplied by different factors for moving up the slope ($f_{up}$) down the slope ($f_{down}$) and moving while carrying a resource ($f_{carry}$). 
An agent moving sideways on the slope pays the same cost as one moving on a non-slope area in any direction.
An agent moving one tile up the slope at time step $t=0$ while carrying a resource, for example, pays a cost of $C_{0} = f_{up} \cdot f_{carry} \cdot c$ .
There are $n_{resources}$ resources at the source, initially.
Every time a resource is removed from the source, another one appears at the source so there are always at least $n_{resources}$.
Each resource retrieved provides all team members a reward of $R$.
The values we chose for these parameters can be found in the appendix\\

Fitness can be calculated for the team or for an individual depending on the level of selection.
When calculating fitness for a team of agents, the fitness function is as follows:\\ 
\\
$F = \sum_{t=1}^{T} \sum_{i}^{n_{agents}} (R_{ti} - C_{ti}) $\\
\\
That is, for each agent, at each time step, we calculate the reward it received at that time step (whether from retrieving a resource itself or from another agent retrieving a resource) and we subtract the cost it individually paid at that time step. 
We then take the summation of this calculation for all agents over all time steps in the simulation.
The reward and cost for an agent $i$ at time step $t$ can be computed as shown here:\\
\\
$
R_{ti} = R \cdot r_{t}
$\\
\\
where $r_t$ is the total number of resources retrieved by all agents at time $t$
\\
\\
\\
$
C_{ti} = \left\{
        \begin{array}{ll}
            c & \quad not on slope or moved sideways on slope\\
            c \cdot f_{up} & \quad up slope\\
            c \cdot f_{down} & \quad down slope\\
            c \cdot f_{carry} & \quad not on slope or moved sideways on slope while carrying\\
            c \cdot f_{carry} \cdot f_{up} & \quad up slope while carrying \\
            c \cdot f_{carry} \cdot f_{down} & \quad down slope while carrying\\
        \end{array}
    \right.
$
\\
\\
When calculating fitness for an individual agent, the fitness function is as follows:\\
\\
$F = \sum_{t=1}^{T} (R_{ti} - C_{ti}) $
\\
\\
Example: Agent 1 incurs -200 retrieving resource. 
Agent 2 incurs -100 wandering around cache. 
Agent 1 score= 1000 - 200. 
Agent 2 score = 1000 - 100.
Team selection: team score = agent 1 score + agent 2 score = 800 + 900 = 1700
Individual selection: agent 1 score = 800, agent 2 score = 900. 
In team selection, each team is compared to other teams. 
In individual selection, the highest scoring individual is selected. 

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{arena.jpg}
	\caption{Arena Layout}
	\label{fig:arena}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.3\textwidth]{arena_layout.png}
	\caption{Arena Screenshot}
	\label{fig:arena_2}
\end{figure}

\subsection{Observations and Actions}\label{observation_space}

Agents have a sensing range that indicates how many tiles around them they can observe. 
A sensing range of 0 means an agent can just observe the current tile it is on. 
A range of 1 means it can observe a square centred  on its location that extends 1 tile in each direction (9 tiles total including current tile). 
A range of 2 means it can observe a square extending 2 tiles in each direction (25 tiles total). 
And so on.
We assume our agent represents a robot with only local sensing capabilities and use a sensing range of 1, which has the added benefit of reducing computation.
For each tile in its sensing range, an agent observes a onehotencoded 4-bit vector. 
The values it reads denote the following: Blank= 1000, Agent = 0100, Resource = 0010, Wall = 0001.
Tiles are read row by row from top left to bottom right. 
The next part of an agent's observation is a 4-bit vector denoting which part of the arena it is on, similar to a real robot with a ground sensor that can detect the unique colour of each area.
The values of this vector can be as follows: Nest = 1000, Cache = 0100, Slope = 0010, Source = 0001
The final part of an agent's observation is 1-bit for resource possession. 
The values can be as follows: Has resource = 1, Doesn’t have resource = 0
The total length of the observation vector is 9x4 + 4 + 1= 41 bits.\\

An agent can perform 6 possible actions, represented by the following values: Forward = 0, Backward = 1, Left = 2, Right = 3, Pick-up = 4, Drop = 5.
We use a recurrent neural network to choose actions based on the observed state.
Since many of the positions in the environment will produce the same observation, a recurrent neural network gives the agent a simple form of memory, preventing it from getting "stuck" in infinite state transition loops.
The default network has 41 inputs, 1 bias input and 6 recurrent inputs (one for each of the 6 outputs). 
There is no hidden layer, just a 6-neuron output layer. 
This makes for a total of (41+1+6)x6 = 288 weights. 
The output layer uses a linear activation function.

\subsection{Team Type and Reward Level}\label{rewards}

During the evolutionary process, it is possible to have four different combinations of team type and reward level that impact evolution.
A team can be either homogeneous, with all agents having the same genome, or it can be heterogeneous, with agents having different genomes.
In our case, a heterogeneous team has two different genomes, with half the team having one genome and the other half of the team having the other.
During evolution, agents can be rewarded as a team or individually, meaning the resource retrieved by an agent can, respectively, count towards the fitness of its teammates or only its own fitness.
In the latter case, an individual agent with a high reward and low cost can be selected while its team-mates are discarded. 
The four combinations are thus: heterogeneous team with team rewards (Het-Team), homogeneous team with team rewards (Hom-Team), heterogeneous team with individual rewards (Het-Ind) and homogeneous team with individual rewards (Hom-Ind).\\

For the purposes of this analysis, we only use the Hom-Team configuration.
\end{appendices}
\end{document}
