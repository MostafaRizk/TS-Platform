\documentclass[sigconf]{aamas}  % do not change this line!
\usepackage{balance}  % do not change this line -- unless you manually balance your last page

\usepackage{booktabs}
\usepackage{amsmath}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

%% do not change the following lines
\setcopyright{ifaamas}  % do not change this line!
\acmConference[AAMAS'19]{Proc.\@ of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019)}{May 13--17, 2019}{Montreal, Canada}{N.~Agmon, M.~E.~Taylor, E.~Elkind, M.~Veloso (eds.)}  % do not change this line!
\acmYear{2019}  % do not change this line!
\copyrightyear{2019}  % do not change this line!

%% the rest of your preamble here

\settopmatter{printacmref=true}
  % mandatory for ACM publications, do not delete

\fancyhead{}
  % do not delete this code.

\usepackage{balance}
  % for creating a balanced last page (usually end of the references)

%% the rest of your preamble here

\newenvironment{psmallmatrix}
  {\bigg(\begin{smallmatrix}}
  {\end{smallmatrix}\bigg)}
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Effective Evolution of Task Specialisation in Multi-Robot Teams}  

 \author{Mostafa Rizk}
 \affiliation{%
  \institution{Faculty of Information Technology, Monash University}
  \city{Melbourne} 
  \state{Australia} 
 }
 \email{mostafa.rizk@monash.edu}

 \author{Julian Garc\'ia}
 \affiliation{%
  \institution{Faculty of Information Technology, Monash University}
  \city{Melbourne} 
  \state{Australia} 
 }
 \email{julian.garcia@monash.edu}

 \author{Aldeida Aleti}
 \affiliation{%
  \institution{Faculty of Information Technology, Monash University}
  \city{Melbourne} 
  \state{Australia} 
 }
 \email{aldeida.aleti@monash.edu}
 
  \author{Giuseppe Cuccu}
 \affiliation{%
 	\institution{eXascale Infolab, Department of Computer Science, University of Fribourg}
 	\city{Fribourg} 
 	\state{Switzerland} 
 }
 \email{giuseppe.cuccu@unifr.ch}

%% The example's default list of authors is too long for headers
%\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}  % put your abstract here!

TODO

\end{abstract}


% AAMAS: the ACM CCS are not needed within AAMAS papers
%% The code below should be generated by the tool at
%% http://dl.acm.org/ccs.cfm
%% Please copy and paste the code instead of the example below. 
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010219.10010220</concept_id>
<concept_desc>Computing methodologies~Multi-agent systems</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010178.10010219.10010223</concept_id>
<concept_desc>Computing methodologies~Cooperation and coordination</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010341</concept_id>
<concept_desc>Computing methodologies~Modeling and simulation</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Multi-agent systems}
\ccsdesc[500]{Computing methodologies~Cooperation and coordination}
\ccsdesc[500]{Computing methodologies~Modeling and simulation}


\keywords{Evolutionary algorithms; Cooperation; Division of labour}  % put your semicolon-separated keywords here!

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% start of main body of paper

\section{Introduction}

Aim is to evolve a multi-agent team that solves a task in a partitioned way. 
Also to understand the qualities of the fitness landscape in such a task for each of four evolutionary configurations. 

\section{Background}

Khamis et al review multi-robot task allocation. Evolution is one approach. Less manual programming. In theory, don’t need to analyse each new task.
Pini et al manually programmed agents that alternate between generalist and specialist strategy.
Ferrante evolved specialisation 
Waibel et al studied 4 evolutionary configurations

\section{Experimental Methods}

\subsection{A foraging task with stag-hunt like properties}

\todo{Describe the payoffs, the arena, etc... use a bit of mathematics, figures}

You have a team of $n$ robots. They are placed in an rectangular arena  with $l$ tiles and $w$ tiles. Each episode is composed of a number of finite time-steps $t=0, .... t_{max}$.
You pay a cost....

Use a foraging task as testbed for evolution of specialisation.
Task modeled off Atta Leafcutter ant, as in Ferrante et al and Pini et al.
Ants retrieve leaves from tree and take them to nest. 
Sometimes ants partition the task. Some ants are droppers who cut leaves and let them fall, other ants are collectors who collect from the base of the tree to the nest.
This specialisation is advantageous because gravity transports leaves faster than ants can.
Ants are able to transport more leaves in the same time-span while consuming less energy.
We model this similarly to Ferrante et al, with a slope.
Like ants, robots/agents transport resources from source to nest. Source is top of slope. Nest is bottom.
Agent teams can use generalist strategy, each acts individually, going up and down the slope to retrieve resources.
Agent teams can use specialist strategy, where part of the team drops things from the top of the slope and part collects resources that accumulate at the base of the slope, called the cache.
Much like real robots and ants, there is a cost to moving (depleting battery or energy reserves) and it is compounded when going up the slope (simulates gravity). 
Much like the ants, a specialist strategy would conserve energy/cost and resources slide down faster than they can be carried. Preliminary analysis with hardcoded agents verifies that specialist teams gain higher overall reward than generalist teams.
This can be considered a stag hunt game since an agent only benefits from adopting a specialist strategy if its team member also adopts a specialist strategy. 

\subsection{Simulation Environment}

L x W tiles with source, slope, cache, nest.
We use L=8 and W=4.
Nest=1, Cache=2, Slope=4, Source=1
Resources appear in source and replenish when removed. Always at least 3. 2 agents.
Resources disappear at nest.
3D physics is computationally expensive. Use simplified version to expedite experiments as focus of research is on evolutionary process and team dynamics rather than robotic element.
Resources slide on slope with speed of 4 tiles/time step.
Robots travel at 1 tile per time step.


\subsection{Rewards and Costs}

To simulate battery, agents pay costs for moving.
Up cost 3 down cost 0.2 carry factor 2 reward factor 1000. Base cost is 1.
If not carrying: Movement cost is -1, up slope is -3, down slope is -0.2
If carrying: Movement cost is -2, up slope is -6, down slope is -0.4
Both agents get +1000 if either retrieves a resource. Reward is shared but cost is individual.
Example: Agent 1 incurs -200 retrieving resource. Agent 2 incurs -100 wandering around cache. Agent 1 score= 1000 - 200. Agent 2 score = 1000 - 100.
Team selection: team score = agent 1 score + agent 2 score = 800 + 900 = 1700
Individual selection: agent 1 score = 800, agent 2 score = 900. 
In team selection, team is compared to other teams. In individual selection, highest scoring individual is selected. 

\subsection{Observations and Actions}

Agents can sense and act. Use recurrent neural network.
Agents have sensing range. 0 means just current tile. 1 means square that extends 1 tile in each direction (9 tiles total including current tile). 2 means square extending 2 tiles (25 tiles). We use sensing range 1 to reduce computation.
For each tile, agent observes a onehotencoded 4-bit vector. 
Blank= 1000, Agent = 0100, Resource = 0010, Wall = 0001
Tiles read from top left to bottom right. In this case, 4x9 bits so far
Next is 4-bit vector for current area
Nest = 1000, Cache = 0100, Slope = 0010, Source = 0001
Next is 1-bit for resource possession. Has resource = 1. Doesn’t have resource = 0
Total bits in observation are 9x4 + 4 + 1= 41 bits
6 possible actions:
Forward = 0, Backward = 1, Left = 2, Right = 3, Pick-up = 4, Drop = 5
Recurrent neural network has 41 inputs, 1 bias input and 6 recurrent inputs (one for each of the 6 outputs). There is no hidden layer, just a 6-neuron output layer. This makes for a total of (41+1+6)x6 = 288 weights. The output layer uses a linear activation function.

\subsection{Evolutionary Algorithm}

Use CMA-ES algorithm. Traditional GA maintains a population of solutions. CMA-ES maintains a probability distribution with a mean and sigma. Sample any number of individuals from this distribution. Fitness of those individuals is used to update the mean and sigma, while keeping track of the best solution so far. CMA parameters set according to CMA-ES defaults enumerated here. Change the following:
Maxiter = 5000 generations, pop size = 40 (80 for Het-Ind and Hom-Ind), tolx = 1e-3, tolfunhist = 2e2
Population size is different because number of teams is held constant (40 teams). 
If team level selection, genome contains all team members. When algorithm selects genome, it selects whole team.
If individual level selection, genome contains one individual. When algorithm selects genome, it selects one member of a team. There are two agents on each team. There are two genomes per team thus 80 genomes in the population.
If x changes by less than tolx, the algorithm terminates. This ensures that the algorithm stops if the genomes aren’t being changed by much. 
The history variable stores the best fitness at a generation. The difference between the minimum and maximum values in the history is calculated. If the difference is less than tolfunhist, the algorithm terminates. Essentially: if the best fitness varies by less than 2000 over 9 generations, the algorithm terminates. 
We bootstrap evolution using random weight guessing. 
Running evolution on its own we find that the algorithm suffers from flat fitness landscape.
Bootstrapping allows us to start evolution from a genome that is slightly better than random behaviour to bypass the flat fitness issue.
In RWG, we do 10,000 runs of the environment with different seeds with 5 trials for each seed. Agent and resource positions are randomised in each trial.
The best genome found at the end of rwg is returned and used as the mean for CMA-ES.

\subsection{Team Composition}

4 different combinations of team composition/level of selection
Het-Team, Hom-Team, Het-Ind, Hom-Ind.
Het-Team= Different controllers. Assess together. Genome contains two sets of weights (288x2)
Hom-Team= Same controller twice. Assess together. Genome contains one set of weights (288). Put controller on both agents.
Het-Ind= Different controllers. Assess separately. Genome contains one set of weights (288). Put odd controller on one agent and even controller on the other.
Hom-Ind= Same controller. Assess individually. Genome contains one set of weights (288). Create a copy of the genome. One agent gets the original. Other agent gets the copy. 
Previous study suggests Het-Ind and Hom-Team are most commonly used.
Hom-Team and Hom-Ind perform equally well usually. Het-Ind is sometimes on par but often worse. Het-Team performs poorly across the board.
However, this assumes task that doesn’t benefit from specialisation. 
We believe that for a task that benefits from specialisation, Het-Team will perform as well as homogeneous counterparts (if not better), with Het-Ind being the poor performer.
Intuition is that if 2 codependent roles (dropper and collector), they must be assessed together. If they are assessed separately, good teams are repeatedly split up and ultimately generalist solutions will be more successful in this evolutionary environment.
Also suspect that Hom-Ind will achieve similar results to Hom-Team. May be slower because double population?

\section{Experiments}

We run bootstrapping and evolution for 50 seeds for each of the four configurations. A total of 200 experiments. 
We sample the generated genomes and their logged fitnesses throughout the process and then use flacco to model the fitness landscape based on this data.\\

[Fitness landscape for each configuration]

\section{Discussion}

[insert brilliant groundbreaking discoveries here]

\section{Conclusion}

[insert poignant reflection on groundbreaking discoveries here]


\bibliographystyle{ACM-Reference-Format}  % do not change this line!
\balance  % do not change this line -- unless you manually balance your last page
\bibliography{references,et}  % put name of your .bib file here

\end{document}